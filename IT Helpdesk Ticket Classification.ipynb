{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IT Helpdesk Ticket Classification\n",
    "The solution handles the misclassification of tickets using multifactor AI/ML based classification model. The As-Is classification scheme leads to higher MTTR(Mean Time to Resolution) and low FCR (First Call Resolution) due to misclassification of tickets. The current scheme handles these problems by automating classification of tickets using classification models. The solution considers factors such as ticket impact, urgency, priority along with ticket description and ticket categories.\n",
    "\n",
    "This sample notebook shows you how to deploy IT Helpdesk Ticket Classification Algorithm using Amazon SageMaker.\n",
    "\n",
    "> **Note**: This is a reference notebook and it cannot run unless you make changes suggested in the notebook.\n",
    "\n",
    "#### Pre-requisites:\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. To deploy this ML model successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "    2. or your AWS account has a subscription to IT Helpdesk Ticket Classification. If so, skip step: [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "\n",
    "#### Contents:\n",
    "1. [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "2. [Create an endpoint and perform real-time inference](#2.-Create-an-endpoint-and-perform-real-time-inference)\n",
    "   1. [Create an endpoint](#A.-Create-an-endpoint)\n",
    "   2. [Create input payload](#B.-Create-input-payload)\n",
    "   3. [Perform real-time inference](#C.-Perform-real-time-inference)\n",
    "   4. [Output Result](#D.-Output-Result)\n",
    "   5. [Delete the endpoint](#E.-Delete-the-endpoint)\n",
    "3. [Perform batch inference](#3.-Perform-batch-inference) \n",
    "4. [Clean-up](#4.-Clean-up)\n",
    "    1. [Delete the model](#A.-Delete-the-model)\n",
    "    2. [Unsubscribe to the listing (optional)](#B.-Unsubscribe-to-the-listing-(optional))\n",
    "    \n",
    "\n",
    "#### Usage instructions\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Subscribe to the model package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To subscribe to the model package:\n",
    "1. Open the algorithm listing page **IT Helpdesk Ticket Classification**\n",
    "1. On the AWS Marketplace listing, click on the **Continue to subscribe** button.\n",
    "1. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you and your organization agrees with EULA, pricing, and support terms. \n",
    "1. Once you click on **Continue to configuration button** and then choose a **region**, you will see a **Product Arn** displayed. This is the model package ARN that you need to specify while creating a deployable model using Boto3. Copy the ARN corresponding to your region and specify the same in the ARN cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Usage Instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deployed solution has these **2 steps**: Training the algorithm and Predicting\n",
    "* The system trains on user provided text dataset.\n",
    "* The dataset must contain 2 files - **train.csv** and **predict.csv** .\n",
    "* The machine learning model is trained in the training step.\n",
    "* The testing API takes a csv file **predict.csv** from which the data points are sampled for human annotation.\n",
    "* Create folder **data**,\n",
    "* Inside **data** folder create two folders **training** and **prediction_data**.\n",
    "* Put **train.csv** inside **training** folder and **predict.csv** inside **prediction_data** folder.    \n",
    "* In the usage instruction notebook, the detailed steps are mentioned to before each cell.\n",
    "<br>\n",
    "\n",
    "**train.csv**\n",
    "* Put **train.csv** inside **training** folder\n",
    "* Name of the file: <b>train.csv</b><br>\n",
    "This file contains historical incidents that have been resolved. The solution uses the following incident specific inputs to derive specific productivity measures such as efficiency, experience and workload management across incident types for incident managers to make the predictions.<br><br>\n",
    "\n",
    "\n",
    "</ul>\n",
    "<li>  ID: Unique identifier for the request- alphanumeric e.g. INC0001029696</li>\n",
    "<li> Reported_Day: The day of the week in number (Preferred format: 1-7)</li>\n",
    "\n",
    "<li> prod_cat: First level category for requests e.g. Miscellaneous_Instance_Database_SQL Server Database</li>\n",
    "<li> Country: Country of origin of request, Preferred format: USA)</li>\n",
    "<li> Detailed_Description: Free Text Describing the problem in users works</li>\n",
    "<li> Priority: Status of the request e.g. Low/Medium/High</li>\n",
    "<li> Impact: High/Medium/Low\n",
    "<li> Incident_Type: Type of incident\n",
    "<li> Reported_Source: Source of report\n",
    "<li> Target: Target variable\n",
    "</ul><br>\n",
    ,
    "</ul>\n",
    "<li> </li>\n",
    "\n",
    "</ul>\n",
    "\n",
    "**predict.csv**\n",
    "\n",
    "* Put **predict.csv** inside **prediction_data** folder\n",
    "<li> The format of predict.csv is similar to train.csv \n",
    "</b> and The solution uses the following incident specific inputs to derive specific productivity measures such as efficiency, experience and workload management across incident types for incident managers to make the predictions.<br><br>\n",
    "</ul>\n",
    "<li>  ID: Unique identifier for the request- alphanumeric e.g. INC0001029696</li>\n",
    "<li> Reported_Day: The day of the week in number (Preferred format: 1-7)</li>\n",
    "\n",
    "<li> prod_cat: First level category for requests e.g. Miscellaneous_Instance_Database_SQL Server Database</li>\n",
    "<li> Country: Country of origin of request, Preferred format: USA)</li>\n",
    "<li> Detailed_Description: Free Text Describing the problem in users works</li>\n",
    "<li> Priority: Status of the request e.g. Low/Medium/High</li>\n",
    "<li> Impact: High/Medium/Low\n",
    "<li> Incident_Type: Type of incident\n",
    "<li> Reported_Source: Source of report\n",
    "\n",
    "#### Invoking endpoint\n",
    "##### AWS CLI Command\n",
    "If you are using real time inferencing, please create the endpoint first and then use the  following command to invoke it:\n",
    "``` bash \n",
    "aws sagemaker-runtime invoke-endpoint --endpoint-name \"endpoint-name\" --body fileb://$file_name --content-type text/csv --accept application/output.csv\n",
    "```\n",
    "Substitute the following parameters:\n",
    "* `\"endpoint-name\"` - name of the inference endpoint where the model is deployed.\n",
    "* `file_name` - Path of the directory where train.csv and predict.csv are placed\n",
    "* `text/csv` - type of the given input file.\n",
    "* `output.csv` - filename where the inference results are written to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64 \n",
    "import uuid\n",
    "from sagemaker import ModelPackage\n",
    "import sagemaker as sage\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import ModelPackage\n",
    "import boto3\n",
    "from IPython.display import Image\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from numpy import asarray\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "\n",
    "sagemaker_session = sage.Session()\n",
    "\n",
    "bucket=sagemaker_session.default_bucket()\n",
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 prefixes\n",
    "common_prefix = \"help-desk\"\n",
    "training_input_prefix = common_prefix + \"/training-input-data\"\n",
    "batch_inference_input_prefix = common_prefix + \"/batch-inference-input-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sage.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#please maintaine this folder structure... put your \"\"train.csv\"\" files inside \"\"training\"\" folder of \"\"data\"\" folder\n",
    "TRAINING_WORKDIR = \"data/training\"\n",
    "\n",
    "TRAINING_DATA = TRAINING_WORKDIR + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_WORKDIR = \"data/training\"\n",
    "\n",
    "# training input location\n",
    "training_input = sagemaker_session.upload_data(TRAINING_WORKDIR, key_prefix=training_input_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from sagemaker.algorithm import AlgorithmEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Algorithm ARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_arn ='' # put your ARN in between  single quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = AlgorithmEstimator(\n",
    "    algorithm_arn=algorithm_arn,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c4.xlarge',\n",
    "    base_job_name='help-desk-marketplace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Now run the training job using algorithm arn %s in region %s\" % (algorithm_arn, sagemaker_session.boto_region_name))\n",
    "algo.fit({'training': training_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "#prediction data location\n",
    "#please maintaine this folder structure... put your \"\"predict.csv\"\" files inside \"\"prediction_data\"\" folder of \"\"data\"\" folder\n",
    "TRANSFORM_WORKDIR = \"data/prediction_data\"\n",
    "filename = os.path.join(TRANSFORM_WORKDIR, \"predict.csv\")\n",
    "\n",
    "\n",
    "api_inputfile = \"predict.csv\"\n",
    "filepath = os.path.join(os.getcwd(), os.path.join(TRANSFORM_WORKDIR, api_inputfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(filename)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Perform batch inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will perform batch inference using multiple input payloads together. If you are not familiar with batch transform, and want to learn more, see these links:\n",
    "1. [How it works](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-batch-transform.html)\n",
    "2. [How to run a batch transform job](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORM_WORKDIR = \"data/prediction_data\"\n",
    "transform_input = sagemaker_session.upload_data(TRANSFORM_WORKDIR, key_prefix=batch_inference_input_prefix) + \"/predict.csv\"\n",
    "print(\"Transform input uploaded to \" + transform_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = algo.transformer(1, 'ml.m4.xlarge')\n",
    "transformer.transform(transform_input, content_type='text/csv')\n",
    "transformer.wait()\n",
    "\n",
    "print(\"Batch Transform output saved to \" + transformer.output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the Batch Transform Output in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "parsed_url = urlparse(transformer.output_path)\n",
    "bucket_name = parsed_url.netloc\n",
    "file_key = '{}/{}.out'.format(parsed_url.path[1:], api_inputfile)\n",
    "\n",
    "s3_client = sagemaker_session.boto_session.client('s3')\n",
    "\n",
    "response = s3_client.get_object(Bucket = sagemaker_session.default_bucket(), Key = file_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketFolder = transformer.output_path.rsplit('/')[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3_conn = boto3.client(\"s3\")\n",
    "bucket_name=sagemaker_session.default_bucket()\n",
    "with open('output.csv', 'wb') as f:\n",
    "    s3_conn.download_fileobj(bucket_name, bucketFolder+'/' + api_inputfile +'.out', f)\n",
    "    print(\"Output file loaded from bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.read_csv('output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Create an endpoint and perform real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to understand how real-time inference with Amazon SageMaker works, see [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='it-help-desk'\n",
    "\n",
    "content_type='text/csv'\n",
    "\n",
    "real_time_inference_instance_type='ml.m5.xlarge'\n",
    "batch_transform_inference_instance_type='ml.m5.large'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Algorithm ARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_arn ='' # put your ARN in between  single quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Create an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_wrapper(endpoint, session):\n",
    "    return sage.predictor.RealTimePredictor(endpoint, session,content_type)\n",
    "\n",
    "#create a deployable model from the model package.\n",
    "\n",
    "model = ModelPackage(role=role,\n",
    "                    model_package_arn=algorithm_arn,\n",
    "                    sagemaker_session=sagemaker_session,\n",
    "                    predictor_cls=predict_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deploy the model\n",
    "predictor = algo.deploy(1, 'ml.m4.xlarge',endpoint_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once endpoint has been created, you would be able to perform real-time inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Perform real-time inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"data/prediction_data/predict.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws sagemaker-runtime invoke-endpoint \\\n",
    "    --endpoint-name 'it-help-desk' \\\n",
    "    --body fileb://$file_name \\\n",
    "    --content-type 'text/csv' \\\n",
    "    --region us-east-2 \\\n",
    "    'output.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.read_csv('output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Unsubscribe to the listing (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to unsubscribe to the model package, follow these steps. Before you cancel the subscription, ensure that you do not have any [deployable model](https://console.aws.amazon.com/sagemaker/home#/models) created from the model package or using the algorithm. Note - You can find this information by looking at the container name associated with the model. \n",
    "\n",
    "**Steps to unsubscribe to product from AWS Marketplace**:\n",
    "1. Navigate to __Machine Learning__ tab on [__Your Software subscriptions page__](https://aws.amazon.com/marketplace/ai/library?productType=ml&ref_=mlmp_gitdemo_indust)\n",
    "2. Locate the listing that you want to cancel the subscription for, and then choose __Cancel Subscription__  to cancel the subscription.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
